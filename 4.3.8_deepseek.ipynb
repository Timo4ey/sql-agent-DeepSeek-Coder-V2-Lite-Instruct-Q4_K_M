{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/llm_api_server/app/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3517: UserWarning: WARNING! device is not default parameter.\n",
      "                device was transferred to model_kwargs.\n",
      "                Please confirm that device is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/workspaces/llm_api_server/app/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3517: UserWarning: WARNING! flash_attn is not default parameter.\n",
      "                flash_attn was transferred to model_kwargs.\n",
      "                Please confirm that flash_attn is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/workspaces/llm_api_server/app/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3517: UserWarning: WARNING! min_p is not default parameter.\n",
      "                min_p was transferred to model_kwargs.\n",
      "                Please confirm that min_p is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "llama_model_loader: loaded meta data with 42 key-value pairs and 377 tensors from /opt/models/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = deepseek2\n",
      "llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct\n",
      "llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\n",
      "llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\n",
      "llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\n",
      "llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\n",
      "llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\n",
      "llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\n",
      "llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\n",
      "llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\n",
      "llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\n",
      "llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\n",
      "llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\n",
      "llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\n",
      "llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\n",
      "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\n",
      "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  37:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  38:                      quantize.imatrix.file str              = /models/DeepSeek-Coder-V2-Lite-Instru...\n",
      "llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  40:             quantize.imatrix.entries_count i32              = 293\n",
      "llama_model_loader: - kv  41:              quantize.imatrix.chunks_count i32              = 139\n",
      "llama_model_loader: - type  f32:  108 tensors\n",
      "llama_model_loader: - type q5_0:   14 tensors\n",
      "llama_model_loader: - type q8_0:   13 tensors\n",
      "llama_model_loader: - type q4_K:  229 tensors\n",
      "llama_model_loader: - type q6_K:   13 tensors\n",
      "llm_load_vocab: control-looking token: 100004 '<｜fim▁end｜>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: control-looking token: 100003 '<｜fim▁begin｜>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: control-looking token: 100002 '<｜fim▁hole｜>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: control token: 100004 '<｜fim▁end｜>' is not marked as EOG\n",
      "llm_load_vocab: control token: 100003 '<｜fim▁begin｜>' is not marked as EOG\n",
      "llm_load_vocab: control token: 100008 '<|EOT|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
      "llm_load_vocab: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
      "llm_load_vocab: control token: 100002 '<｜fim▁hole｜>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 2400\n",
      "llm_load_vocab: token to piece cache size = 0.6661 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = deepseek2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 102400\n",
      "llm_load_print_meta: n_merges         = 99757\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 163840\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 27\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 192\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 10944\n",
      "llm_load_print_meta: n_expert         = 64\n",
      "llm_load_print_meta: n_expert_used    = 6\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = yarn\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.025\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 16B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 15.71 B\n",
      "llm_load_print_meta: model size       = 9.65 GiB (5.28 BPW) \n",
      "llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct\n",
      "llm_load_print_meta: BOS token        = 100000 '<｜begin▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOS token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOT token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: PAD token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: LF token         = 126 'Ä'\n",
      "llm_load_print_meta: FIM PRE token    = 100003 '<｜fim▁begin｜>'\n",
      "llm_load_print_meta: FIM SUF token    = 100002 '<｜fim▁hole｜>'\n",
      "llm_load_print_meta: FIM MID token    = 100004 '<｜fim▁end｜>'\n",
      "llm_load_print_meta: EOG token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_print_meta: n_layer_dense_lead   = 1\n",
      "llm_load_print_meta: n_lora_q             = 0\n",
      "llm_load_print_meta: n_lora_kv            = 512\n",
      "llm_load_print_meta: n_ff_exp             = 1408\n",
      "llm_load_print_meta: n_expert_shared      = 2\n",
      "llm_load_print_meta: expert_weights_scale = 1.0\n",
      "llm_load_print_meta: expert_weights_norm  = 0\n",
      "llm_load_print_meta: expert_gating_func   = softmax\n",
      "llm_load_print_meta: rope_yarn_log_mul    = 0.0707\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 376 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  9880.47 MiB\n",
      "warning: failed to mlock 293994496-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".....................................................................................\n",
      "llama_new_context_with_model: flash_attn requires n_embd_head_k == n_embd_head_v - forcing off\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 8192\n",
      "llama_new_context_with_model: n_ctx_per_seq = 8192\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (8192) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 27, can_shift = 0\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 3072, n_embd_v_gqa = 2048\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2160.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2160.00 MiB, K (f16): 1296.00 MiB, V (f16):  864.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.39 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1924\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '139', 'quantize.imatrix.entries_count': '293', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '100001', 'deepseek2.block_count': '27', 'deepseek2.leading_dense_block_count': '1', 'deepseek2.attention.kv_lora_rank': '512', 'deepseek2.rope.scaling.original_context_length': '4096', 'general.file_type': '15', 'quantize.imatrix.file': '/models/DeepSeek-Coder-V2-Lite-Instruct-GGUF/DeepSeek-Coder-V2-Lite-Instruct.imatrix', 'deepseek2.attention.head_count': '16', 'deepseek2.feed_forward_length': '10944', 'deepseek2.attention.head_count_kv': '16', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\\n\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\", 'deepseek2.context_length': '163840', 'deepseek2.embedding_length': '2048', 'deepseek2.expert_used_count': '6', 'deepseek2.attention.key_length': '192', 'tokenizer.ggml.pre': 'deepseek-llm', 'deepseek2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'deepseek2', 'deepseek2.vocab_size': '102400', 'tokenizer.ggml.eos_token_id': '100001', 'deepseek2.expert_shared_count': '2', 'deepseek2.rope.freq_base': '10000.000000', 'deepseek2.attention.value_length': '128', 'deepseek2.expert_feed_forward_length': '1408', 'deepseek2.rope.dimension_count': '64', 'deepseek2.expert_count': '64', 'deepseek2.rope.scaling.type': 'yarn', 'deepseek2.rope.scaling.factor': '40.000000', 'deepseek2.rope.scaling.yarn_log_multiplier': '0.070700', 'deepseek2.expert_weights_scale': '1.000000', 'tokenizer.ggml.model': 'gpt2', 'general.name': 'DeepSeek-Coder-V2-Lite-Instruct', 'tokenizer.ggml.bos_token_id': '100000'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
      "\n",
      "' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
      "\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n",
      "Using chat eos_token: <｜end▁of▁sentence｜>\n",
      "Using chat bos_token: <｜begin▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "\n",
    "\n",
    "model_path = \"/opt/models/lmstudio-community/Qwen2.5-14B-Instruct-1M-GGUF/Qwen2.5-14B-Instruct-1M-Q4_K_M.gguf\"\n",
    "\n",
    "model_path = \"/opt/models/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf\"\n",
    "model_path = \"/opt/models/lmstudio-community/Qwen2.5-Coder-7B-Instruct-GGUF/Qwen2.5-Coder-7B-Instruct-Q6_K.gguf\"\n",
    "model_path = \"/opt/models/lmstudio-community/Qwen2.5-Coder-14B-Instruct-GGUF/Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf\"\n",
    "model_path = \"/opt/models/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF/qwen2.5-coder-32b-instruct-q3_k_m.gguf\"\n",
    "model_path = \"/opt/models/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF/DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf\"\n",
    "llm: LlamaCpp = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    device=\"cpu\",\n",
    "    n_threads=4,  \n",
    "    n_ctx=8184, \n",
    "    # n_gpu_layers=0  # Если вы не используете GPU, оставьте 0\n",
    "    n_batch=512,  \n",
    "    use_mlock=True,  \n",
    "    # verbose=True,\n",
    "    flash_attn=True,\n",
    "    temperature=0.0,\n",
    "    max_tokens=512,\n",
    "    repeat_penalty=1.1,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    min_p=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "# подключимся к базе\n",
    "db = SQLDatabase.from_uri(\"postgresql+psycopg2://root:mypass@localhost:5432/dvdrental\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Executor\n",
    "\n",
    "template = \"\"\"\n",
    "Execute answer from text and return it in the required format.\n",
    "Before returning answer think about the format.\n",
    "* If it asks you return rate just return number (example: 1.98)\n",
    "* Return only required data\n",
    "Question: {question}\n",
    "Text: {answer}\n",
    "Answer:\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def executor_init(llm: LlamaCpp, prompt_template: PromptTemplate):\n",
    "    def inner(**kwargs):\n",
    "        \"\"\"Executing answer in required format from llm output\"\"\"\n",
    "        return llm.invoke(prompt_template.format(**kwargs))\n",
    "    return inner\n",
    "        \n",
    "llm_executor = executor_init(llm, prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "  llm,\n",
    "  toolkit=toolkit,\n",
    "  verbose=True,\n",
    "  return_exceptions=True,\n",
    "  handle_parsing_errors=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://stepik.org/media/attachments/lesson/1107866/rental_dvd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Llama.generate: 572 prefix-match hit, remaining 45 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    45 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6176.02 ms /    60 tokens\n",
      "Llama.generate: 632 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mactor, address, category, city, country, customer, film, film_actor, film_category, inventory, language, payment, rental, staff, store\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6946.88 ms /   103 tokens\n",
      "Llama.generate: 735 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m The client's email is likely stored in the customer table. I need to query this table to find Enrique Forsythe's email.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT email FROM customer WHERE first_name = 'Enrique' AND last_name = 'Forsythe';\u001b[0m\u001b[36;1m\u001b[1;3m[('enrique.forsythe@sakilacustomer.org',)]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2957.45 ms /    52 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 83 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I have found Enrique Forsythe's email.\n",
      "Final Answer: enrique.forsythe@sakilacustomer.org\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3286.37 ms /    97 tokens\n",
      " 20%|██        | 1/5 [00:19<01:18, 19.54s/it]Llama.generate: 1 prefix-match hit, remaining 626 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   626 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   20625.47 ms /   641 tokens\n",
      "Llama.generate: 642 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mactor, address, category, city, country, customer, film, film_actor, film_category, inventory, language, payment, rental, staff, store\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    43 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4812.92 ms /    82 tokens\n",
      "Llama.generate: 724 prefix-match hit, remaining 686 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m The tables related to films are film, film_actor, film_category. I need to find out the rating column in the film table.\n",
      "Action: sql_db_schema\n",
      "Action Input: film\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE film (\n",
      "\tfilm_id SERIAL NOT NULL, \n",
      "\ttitle VARCHAR(255) NOT NULL, \n",
      "\tdescription TEXT, \n",
      "\trelease_year year, \n",
      "\tlanguage_id SMALLINT NOT NULL, \n",
      "\trental_duration SMALLINT DEFAULT 3 NOT NULL, \n",
      "\trental_rate NUMERIC(4, 2) DEFAULT 4.99 NOT NULL, \n",
      "\tlength SMALLINT, \n",
      "\treplacement_cost NUMERIC(5, 2) DEFAULT 19.99 NOT NULL, \n",
      "\trating mpaa_rating DEFAULT 'G'::mpaa_rating, \n",
      "\tlast_update TIMESTAMP WITHOUT TIME ZONE DEFAULT now() NOT NULL, \n",
      "\tspecial_features TEXT[], \n",
      "\tfulltext TSVECTOR NOT NULL, \n",
      "\tCONSTRAINT film_pkey PRIMARY KEY (film_id), \n",
      "\tCONSTRAINT film_language_id_fkey FOREIGN KEY(language_id) REFERENCES language (language_id) ON DELETE RESTRICT ON UPDATE CASCADE\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from film table:\n",
      "film_id\ttitle\tdescription\trelease_year\tlanguage_id\trental_duration\trental_rate\tlength\treplacement_cost\trating\tlast_update\tspecial_features\tfulltext\n",
      "133\tChamber Italian\tA Fateful Reflection of a Moose And a Husband who must Overcome a Monkey in Nigeria\t2006\t1\t7\t4.99\t117\t14.99\tNC-17\t2013-05-26 14:50:58.951000\t['Trailers']\t'chamber':1 'fate':4 'husband':11 'italian':2 'monkey':16 'moos':8 'must':13 'nigeria':18 'overcom':\n",
      "384\tGrosse Wonderful\tA Epic Drama of a Cat And a Explorer who must Redeem a Moose in Australia\t2006\t1\t5\t4.99\t49\t19.99\tR\t2013-05-26 14:50:58.951000\t['Behind the Scenes']\t'australia':18 'cat':8 'drama':5 'epic':4 'explor':11 'gross':1 'moos':16 'must':13 'redeem':14 'won\n",
      "8\tAirport Pollock\tA Epic Tale of a Moose And a Girl who must Confront a Monkey in Ancient India\t2006\t1\t6\t4.99\t54\t15.99\tR\t2013-05-26 14:50:58.951000\t['Trailers']\t'airport':1 'ancient':18 'confront':14 'epic':4 'girl':11 'india':19 'monkey':16 'moos':8 'must':13 \n",
      "*/\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   686 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    38 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   24270.60 ms /   724 tokens\n",
      "Llama.generate: 1448 prefix-match hit, remaining 53 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to count the occurrences of each rating in the film table.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT rating, COUNT(*) FROM film GROUP BY rating;\u001b[0m\u001b[36;1m\u001b[1;3m[('PG-13', 223), ('G', 178), ('NC-17', 210), ('R', 195), ('PG', 194)]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7810.17 ms /   116 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 104 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m The results show that the most common ratings are 'PG-13' with 223 films, followed by 'G' with 178 films.\n",
      "Final Answer: The most common film rating in the collection is 'PG-13', with a total of 223 films.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3494.81 ms /   108 tokens\n",
      " 40%|████      | 2/5 [01:20<02:11, 43.98s/it]Llama.generate: 1 prefix-match hit, remaining 627 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   627 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18981.04 ms /   642 tokens\n",
      "Llama.generate: 643 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mactor, address, category, city, country, customer, film, film_actor, film_category, inventory, language, payment, rental, staff, store\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4986.33 ms /    84 tokens\n",
      "Llama.generate: 727 prefix-match hit, remaining 217 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m The actor table seems relevant to the question. I should query the schema of the actor table and then search for Johnny Lollobrigida's movies.\n",
      "Action: sql_db_schema\n",
      "Action Input: actor\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE actor (\n",
      "\tactor_id SERIAL NOT NULL, \n",
      "\tfirst_name VARCHAR(45) NOT NULL, \n",
      "\tlast_name VARCHAR(45) NOT NULL, \n",
      "\tlast_update TIMESTAMP WITHOUT TIME ZONE DEFAULT now() NOT NULL, \n",
      "\tCONSTRAINT actor_pkey PRIMARY KEY (actor_id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from actor table:\n",
      "actor_id\tfirst_name\tlast_name\tlast_update\n",
      "1\tPenelope\tGuiness\t2013-05-26 14:47:57.620000\n",
      "2\tNick\tWahlberg\t2013-05-26 14:47:57.620000\n",
      "3\tEd\tChase\t2013-05-26 14:47:57.620000\n",
      "*/\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14920.75 ms /   311 tokens\n",
      "Llama.generate: 1038 prefix-match hit, remaining 301 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m Now I need to find the shortest film with Johnny Lollobrigida.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT f.title, f.length FROM actor a JOIN film_actor fa ON a.actor_id = fa.actor_id JOIN film f ON fa.film_id = f.film_id WHERE a.first_name = 'Johnny' AND a.last_name = 'Lollobrigida';\u001b[0m\u001b[36;1m\u001b[1;3m[('Amadeus Holy', 113), ('Banger Pinocchio', 113), ('Bonnie Holocaust', 63), ('Chitty Lock', 107), ('Commandments Express', 59), ('Coneheads Smoochy', 112), ('Daddy Pittsburgh', 161), ('Daisy Menagerie', 84), ('Enough Raging', 158), ('Escape Metropolis', 167), ('Fire Wolves', 173), ('Frontier Cabin', 183), ('Goodfellas Salute', 56), ('Grail Frankenstein', 85), ('Groove Fiction', 111), ('Hall Cassidy', 51), ('Heavenly Gun', 49), ('Kramer Chocolate', 171), ('Love Suicides', 181), ('Metal Armageddon', 161), ('Pacific Amistad', 144), ('Patton Interview', 175), ('Pocus Pulp', 138), ('Ridgemont Submarine', 46), ('Rings Heartbreakers', 58), ('Smile Earring', 60), ('Soldiers Evolution', 185), ('Star Operation', 181), ('Sunrise League', 135)]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   301 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    33 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13078.38 ms /   334 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 83 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m The shortest film with Johnny Lollobrigida is 'Amadeus Holy' with a length of 113 minutes.\n",
      "Final Answer: Amadeus Holy\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3031.85 ms /    86 tokens\n",
      " 60%|██████    | 3/5 [02:15<01:38, 49.04s/it]Llama.generate: 1 prefix-match hit, remaining 634 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   634 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   20837.37 ms /   649 tokens\n",
      "Llama.generate: 650 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mactor, address, category, city, country, customer, film, film_actor, film_category, inventory, language, payment, rental, staff, store\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7347.91 ms /   115 tokens\n",
      "Llama.generate: 765 prefix-match hit, remaining 136 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "I need to find out where Mike Hillyer works. I should look at the staff table since it contains information about employees. Then, I will query for Mike Hillyer in that table.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT * FROM staff WHERE first_name = 'Mike' AND last_name = 'Hillyer';\u001b[0m\u001b[36;1m\u001b[1;3m[(1, 'Mike', 'Hillyer', 3, 'Mike.Hillyer@sakilastaff.com', 1, True, 'Mike', '8cb2237d0679ca88db6464eac60da96345513964', datetime.datetime(2006, 5, 16, 16, 13, 11, 793280), <memory at 0x74d64b53a800>)]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15412.09 ms /   273 tokens\n",
      "Llama.generate: 1038 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Mike Hillyer works in the company. Now, I need to find out which country he is from. To do this, I will look at the address table and join it with the city table to get the country information.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT a.address, c.city, co.country FROM address a JOIN city c ON a.city_id = c.city_id JOIN country co ON c.country_id = co.country_id WHERE a.address_id IN (SELECT address_id FROM staff WHERE first_name = 'Mike' AND last_name = 'Hillyer');\u001b[0m\u001b[36;1m\u001b[1;3m[('23 Workhaven Lane', 'Lethbridge', 'Canada')]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3462.99 ms /    52 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 88 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Mike Hillyer is from Canada. I have found the country where he works. The final answer is: Canada.\n",
      "Final Answer: Canada\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2757.88 ms /    89 tokens\n",
      " 80%|████████  | 4/5 [03:05<00:49, 49.37s/it]Llama.generate: 1 prefix-match hit, remaining 633 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   633 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19199.92 ms /   648 tokens\n",
      "Llama.generate: 649 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mactor, address, category, city, country, customer, film, film_actor, film_category, inventory, language, payment, rental, staff, store\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5705.13 ms /    94 tokens\n",
      "Llama.generate: 743 prefix-match hit, remaining 686 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "I need to find the average rental rate of films that are less than an hour long. I should look at the film table and see if there's a column for duration in minutes or hours.\n",
      "Action: sql_db_schema\n",
      "Action Input: film\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE film (\n",
      "\tfilm_id SERIAL NOT NULL, \n",
      "\ttitle VARCHAR(255) NOT NULL, \n",
      "\tdescription TEXT, \n",
      "\trelease_year year, \n",
      "\tlanguage_id SMALLINT NOT NULL, \n",
      "\trental_duration SMALLINT DEFAULT 3 NOT NULL, \n",
      "\trental_rate NUMERIC(4, 2) DEFAULT 4.99 NOT NULL, \n",
      "\tlength SMALLINT, \n",
      "\treplacement_cost NUMERIC(5, 2) DEFAULT 19.99 NOT NULL, \n",
      "\trating mpaa_rating DEFAULT 'G'::mpaa_rating, \n",
      "\tlast_update TIMESTAMP WITHOUT TIME ZONE DEFAULT now() NOT NULL, \n",
      "\tspecial_features TEXT[], \n",
      "\tfulltext TSVECTOR NOT NULL, \n",
      "\tCONSTRAINT film_pkey PRIMARY KEY (film_id), \n",
      "\tCONSTRAINT film_language_id_fkey FOREIGN KEY(language_id) REFERENCES language (language_id) ON DELETE RESTRICT ON UPDATE CASCADE\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from film table:\n",
      "film_id\ttitle\tdescription\trelease_year\tlanguage_id\trental_duration\trental_rate\tlength\treplacement_cost\trating\tlast_update\tspecial_features\tfulltext\n",
      "133\tChamber Italian\tA Fateful Reflection of a Moose And a Husband who must Overcome a Monkey in Nigeria\t2006\t1\t7\t4.99\t117\t14.99\tNC-17\t2013-05-26 14:50:58.951000\t['Trailers']\t'chamber':1 'fate':4 'husband':11 'italian':2 'monkey':16 'moos':8 'must':13 'nigeria':18 'overcom':\n",
      "384\tGrosse Wonderful\tA Epic Drama of a Cat And a Explorer who must Redeem a Moose in Australia\t2006\t1\t5\t4.99\t49\t19.99\tR\t2013-05-26 14:50:58.951000\t['Behind the Scenes']\t'australia':18 'cat':8 'drama':5 'epic':4 'explor':11 'gross':1 'moos':16 'must':13 'redeem':14 'won\n",
      "8\tAirport Pollock\tA Epic Tale of a Moose And a Girl who must Confront a Monkey in Ancient India\t2006\t1\t6\t4.99\t54\t15.99\tR\t2013-05-26 14:50:58.951000\t['Trailers']\t'airport':1 'ancient':18 'confront':14 'epic':4 'girl':11 'india':19 'monkey':16 'moos':8 'must':13 \n",
      "*/\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   686 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   27269.25 ms /   759 tokens\n",
      "Llama.generate: 1502 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "I need to calculate the average rental rate of films that are less than an hour long. I should filter the films by their length and then calculate the average rental rate.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT AVG(rental_rate) AS avg_rental_rate FROM film WHERE length < 120;\u001b[0m\u001b[36;1m\u001b[1;3m[(Decimal('2.9263295880149813'),)]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5391.94 ms /    76 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 107 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "The average rental rate of films that are less than an hour long is approximately 2.93.\n",
      "Final Answer: The average rental rate of films that are less than an hour long is approximately 2.93.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   19076.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3745.92 ms /   112 tokens\n",
      "100%|██████████| 5/5 [04:06<00:00, 49.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import  tqdm\n",
    "\n",
    "answers = []\n",
    "\n",
    "for q in tqdm(df[\"question\"]):\n",
    "    llm_answer = agent_executor.invoke(dict(input=q))\n",
    "    clean_answer = llm_executor(**dict(question=llm_answer[\"input\"], answer=llm_answer[\"output\"])).strip()\n",
    "    if clean_answer and clean_answer[0].isdigit():\n",
    "        if \".\" in clean_answer:\n",
    "            clean_answer = float(clean_answer)\n",
    "    \n",
    "    answers.append(dict(question=q, answer=clean_answer) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(answers).rename(columns=dict(input=\"question\",output=\"answer\")).to_csv(\"4.3.8.csv\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
